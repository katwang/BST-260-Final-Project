---
title: 'BST 260 Final Project: Cleaning Physician Compare National Data File'
author: Eunice Yeh
output: html_document
---



(move this to the first chapter of the book) Load `tidyverse` package for data wrangling and plotting functions.
```{r}
library(tidyverse)
```


# Purpose
1. read in and understand the data
2. explore the data and summarize key variables
3. clean up/reduce data: wrangling decisions

## 1. Understanding Practitioner Records

(provide a brief intro/context to this data)

(quick table of metadata)

Import data from a `.csv` file that was downloaded from the (Medicare Data Website)[https://data.medicare.gov/Physician-Compare/Physician-Compare-National-Downloadable-File/].
```{r load-phys-compare, cache=TRUE}
# dat <- read.csv("Physician_Compare_National_Downloadable_File.csv")
# limit = 1000 is the default API if i don't specify anything, so need to add limit=2254703 to get as close to the complete data as possible
# since it is large, will not download certain columns, also specify this in the download link (looked thru metadata on the website to select variables i'm interested in) we've downloaded the csv file before and we know that the exact number of rows is 2254703 so we can specify this in the download link
dat <- read.csv("https://data.medicare.gov/resource/c8qv-268j.csv?$limit=2254703&$select=npi,frst_nm,gndr,cred,med_sch,grd_yr,pri_spec,cty,st,zip,hosp_afl_1,hosp_afl_lbn_1,ehr")
```


```{r}
saveRDS(dat, file="phys_all.rds")
```


Quick look at the data structure:
```{r}
str(dat)
```

We are starting with 2,254,703 records of 41 variables in our practitioner data. To save time and space, the first thing we want to do is to use the process of elimination to remove columns that we know for sure would not inform our wrangling nor the analysis. Since NPI is the only "identification number" we will use to match individual practitioner to the vendor dataset, we can delete all other "identification numbers", i.e., any of the variables that end in `.ID` in `dat`. We also technically don't need the names of each practitioner, but we will keep the First and Last names just to help ease data wrangling (we prefer to read real names than stare at their NPIs as a form of identification per record). The specific address lines are unnecessarily detailed, keeping the City, State, and Zip Code in our dataset will suffice. Not sure what we could use the phone number for. Since we decided not to analyze specific quality measures anymore, we won't need to filter on practitioners who responded yes to the `Reported.Quality.Measures` column. We also don't care about whether or not a practitioner committed to heart health through the Million Hearts Initiative. The rest of the variables seem potentially interesting and/or useful to keep.
```{r}
dat <- dat %>% select(-ends_with(".ID"), -Middle.Name, -Suffix, -Line.1.Street.Address, -Line.2.Street.Address, -Marker.of.address.line.2.suppression, -Phone.Number, -Reported.Quality.Measures, -Committed.to.heart.health.through.the.Million.HeartsÂ..initiative.)
```

We now have 30 somewhat more meaningful variables that can help us understand the data in more depth, so that we can prepare it better for our analyses. Let's continue learning about the structure of our data by answering a few questions:

1. What is the level of uniqueness for each record? If multiple records exist per practitioner, we need to find out which characteristics are defining the uniqueness for each record.
```{r}
dat %>% group_by(NPI) %>% summarize()
```

2. How clean/dirty is our data? Does each of these "more meaningful" variables actually hold valid values across all records?


## 2. Exploring Key Variables of Interest

Look at the distribution of physician graduation year by physician credentials and by use of EHR.
```{r}
gradyr <- dat %>% filter(!is.na(Graduation.year)) %>% 
  group_by(Credential, Used.electronic.health.records) 

gradyr %>% 
  ggplot(aes(Graduation.year)) +
  geom_histogram(color = "black") +
  facet_grid(.~Used.electronic.health.records) +
  ggtitle("Distribution of Grduation Year by Use of EHR")

gradyr %>% filter(Used.electronic.health.records == 'Y') %>%
  ungroup() %>% 
  mutate(Credential = reorder(Credential, Graduation.year)) %>% 
  ggplot(aes(factor(Credential), Graduation.year)) +
  geom_boxplot() + 
  # scale_y_continuous(trans = "sqrt", breaks = seq(0, 100, 2)^2) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.25)) +
  xlab("Credential") +
  ylab("Graduation Year") +
  ggtitle("Dist of Grad Year by Credential among those who used EHR")

gradyr %>%
  ungroup() %>% 
  mutate(Credential = reorder(Credential, Graduation.year)) %>% 
  ggplot(aes(factor(Credential), Graduation.year)) +
  geom_boxplot() + 
  facet_grid(.~Used.electronic.health.records) +
  # scale_y_continuous(trans = "sqrt", breaks = seq(0, 100, 2)^2) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.25)) +
  xlab("Credential") +
  ylab("Graduation Year") +
  ggtitle("Dist of Grad Year by Credential")
```

Look at the distribution of physician graduation year by US States separately for those who used EHR and for those who have not indicated using EHR.
```{r}
# for those who used EHR
gradyr %>% ungroup() %>% filter(Used.electronic.health.records == 'Y') %>% 
  mutate(State = reorder(State, Graduation.year)) %>% 
  ggplot(aes(factor(State), Graduation.year)) +
  geom_boxplot() + 
  # facet_grid(.~Used.electronic.health.records) +
  # scale_y_continuous(trans = "sqrt", breaks = seq(0, 100, 2)^2) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.25)) +
  xlab("US State") +
  ylab("Graduation Year") +
  ggtitle("Dist of Grad Year by State among those who used EHR")

# for those who did not indicate having used EHR
gradyr %>% ungroup() %>% filter(Used.electronic.health.records != 'Y') %>% 
  mutate(State = reorder(State, Graduation.year)) %>% 
  ggplot(aes(factor(State), Graduation.year)) +
  geom_boxplot() + 
  # facet_grid(.~Used.electronic.health.records) +
  # scale_y_continuous(trans = "sqrt", breaks = seq(0, 100, 2)^2) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.25)) +
  xlab("US State") +
  ylab("Graduation Year") +
  ggtitle("Dist of Grad Year by State among those who have not indicated using EHR")
rm(gradyr) # removing after use to save space
```

Calculate the proportion of EHR use by State.
```{r}
EHR_bystate <- dat %>% 
  group_by(State) %>% 
  summarize(EHR_count = sum(Used.electronic.health.records == 'Y'), total = n(), 
            EHR_prop = EHR_count/total)
EHR_bystate
```

Plot the proportion of EHR use by State.
```{r}
EHR_bystate %>% mutate(State = reorder(State, EHR_prop)) %>% 
  ggplot(aes(factor(State), EHR_prop)) +
  geom_point() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.25)) +
  xlab("State") +
  ylab("Proportion of EHR Used") +
  ggtitle("Proportion of EHR Used by State")
rm(EHR_bystate)
```

Calculate the percentage of EHR use by Primary Specialty and by State. -- consider filterning out n() < 50 or 100?
```{r}
EHR_primspec_bystate <- dat %>% 
  filter(!is.na(Graduation.year)) %>% 
  group_by(State, Primary.specialty) %>% 
  summarize(EHR_count = sum(Used.electronic.health.records == 'Y'), total = n(), EHR_percent = EHR_count/total * 100) %>% 
  mutate(Primary.specialty = reorder(Primary.specialty, EHR_percent))
EHR_primspec_bystate
```

Plot the percentage of EHR use by Primary Specialty and by State.
```{r}
library(RColorBrewer)
EHR_primspec_bystate %>% filter(total > 100) %>% 
  ggplot(aes(Primary.specialty, State,  fill = EHR_percent)) +
  geom_tile(color = "grey50") +
  # scale_x_continuous(expand=c(0,0)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
  scale_fill_gradientn(colors = brewer.pal(9, "YlGn")) +  
  theme(panel.grid = element_blank()) +
  ylab("US States") + 
  xlab("Primary Specialty") +
  ggtitle("Percentage of EHR Used by Primary Specialty and States")
rm(EHR_primspec_bystate)
```


## 2. Data Cleaning Decisions:

1. filter out states outside of the main 50 except for DC since there is not a lot of information in those states and the other data we want to merge with do not contain information on these states anyway.

```{r}
unique(dat$State)
data("state")
new_dat <- dat[dat$State %in% unique(state.abb) == "TRUE" | dat$State == "DC", ]
unique(new_dat$State)
```

2. (TAKE THIS OUT LATER) keep only rows with `Professional.accepts.Medicare.Assignment = 'Y'` since we are only interested in practioners who have definitely accepted medicare assignments, and we need to merge with other data that only contain those serving Medicare.

```{r}
rm(dat) # removing previous data to save space and speed up run time
table(new_dat$Professional.accepts.Medicare.Assignment)
filtered_dat <- new_dat %>% filter(Professional.accepts.Medicare.Assignment == 'Y')
table(filtered_dat$Professional.accepts.Medicare.Assignment)
```

3. key variables are NPI, last and first name, gender, credentials, medical school name, grad year, prim specialty, all sec specialties, organization legal name, num of group practice members, city, state, zip code, hospital affiliation CCNs and LBNs, used EHR.

```{r}
filtered_dat <- new_dat # temp, debating if we can just ignore Professional.accepts.Medicare.Assignment
rm(new_dat)
# easier to exclude than to type out all the variables to include
select_dat <- filtered_dat %>% select(-PAC.ID, -Professional.Enrollment.ID, -Middle.Name, -Suffix, -Group.Practice.PAC.ID, -Line.1.Street.Address, -Line.2.Street.Address, -Marker.of.address.line.2.suppression, -Phone.Number, -Professional.accepts.Medicare.Assignment, -Reported.Quality.Measures, -Committed.to.heart.health.through.the.Million.HeartsÂ..initiative.)
str(select_dat)
```

4. merge in accurate city, state, latitude, longitude information by zip codes using the `zipcode` package.

```{r}
rm(filtered_dat)
# separate out the first five zip codes from the last four extension to match R zipcode data
zip_dat <- select_dat %>% mutate(zip = substr(as.character(Zip.Code), 1, 5), zip.ext = substr(as.character(Zip.Code), 6, 9))

# use R zipcode package and data
library(zipcode)
data("zipcode")
zip_dat <- zip_dat %>% left_join(zipcode,by='zip') 
zip_dat %>% select(NPI, Zip.Code, zip, zip.ext, City, State, city, state, latitude, longitude) %>% head(.,20)
```

Check mismatched state between our data and the zipcode data from R.
```{r}
rm(select_dat)

# look up top 10 records with state mismatches
zip_mismatch <- zip_dat %>% 
  mutate(State = as.character(State), state = as.character(state)) %>% 
  filter(State != state) %>% 
  select(NPI, zip, City, city, State, state) %>% 
  group_by(zip, City, State, city, state) %>% 
  summarize(n = n()) %>% 
  arrange(desc(n)) %>% 
  head(5)
zip_mismatch
```

In order to not just blatantly throw away the mismatches, we will look at the top 5 most common mismatches on a case-by-case basis to try to salvage as many records as possible without searching too deeply in our dataset. Also, starting from the 6th most common mismatches, the number of records (n) were down to 58 or less, so they would have little effect on the dataset.

The results of searching up the top 10 mismatches on Google Maps:
- the zipcode 99362 spans across Walla Walla in both WA and OR, so it is likely that the practice is located on the WA side of the zipcode coverage. We will ignore the state mismatch from the zipcode package for this case.
- the zipcode 52761 does belong to Muscatine, IA, which is right on the border of IL. Again, the practice is most likely located on the IA side. Will ignore the state mismatch for this case as well.
- the zip code for Granger, IN was most likely mistyped. The correct zip code for Granger is 46530, which is very close to 43530. We will fix the zipcode for this particular case only.
- the zipcode 22401 does belong to Frederickburg, VA, and is not near the border of VA and MD at all. There is a "Frederick" city in MD, but none of its zipcodes look similar enough to 22401. No clear culprit, will have to let this one go. :(
- The zipcode for Hanover, MD is 21076, which was most likely mistyped as 20176. We will fix this particular case as well.

```{r}
rm(zipcode)

# leave out the 22401 Fredericksburg MD mismatch, fix the zip codes for Granger and Hanover.
zip_fix <- zip_mismatch %>% ungroup() %>% 
  filter(City != 'FREDERICKSBURG') %>% 
  mutate(zip = replace(zip, zip %in% c(43530, 20176), c(46530, 21076))) %>% 
  select(-n)

mismatch_tokeep <- zip_dat %>% 
  select(-zip) %>% # we want to replace the original wrong zip codes with the fixed zip codes
  inner_join(zip_fix, by = c('City','State', 'city', 'state'))

complete_zip <- zip_dat %>%
  mutate(State = as.character(State), state = as.character(state)) %>% 
  filter(State == state) %>% # keeping only the matches
  bind_rows(mismatch_tokeep) # then add in the mismatches we wanted to keep or have fixed
```


5. keep only one record per physician per location (using NPI, first/last name, and City, State and full zip code just to be extra precise) since we will be matching on NPI and zip code with the EHR vendor information dataset and matching on hospital affiliation and state with the hospital data.
```{r}
rm(zip_dat, zip_mismatch, zip_fix, mismatch_tokeep)

# side note: theoretically each physician should only be reporting one primary specialty and should report any additional specialties into the secondary specalty columns, but not every practitioner did that. So as dirty as the data can be, there are indeed a few physicians who have multiple primary specialty records (all distinct columns stayed constant), although the most is having two specialities and there are only 17 of these cases.
complete_zip %>% group_by(NPI, City, State, Zip.Code) %>% filter(n() > 1 | n_distinct(Primary.specialty) > 1) 

unique_dat <- complete_zip %>% distinct(NPI, Last.Name, First.Name, City, State, Zip.Code, .keep_all = TRUE) %>% select(-city, -state) # no longer need the city and state from zipcode package
```
Note that the argument `.keep_all = TRUE` keeps all other variables not considered in the combination of variables that determine the uniqueness. If the combination is not distinct, the first row of values is kept, which is fine because we don't really care as much about the other variables. For example, the organization legal name and city may vary across multiple rows for the same physician who owns a private practice and renames or moves the practice around the area but still within the same state.

6. separate into two datasets by hospital affiliation vs. none for analyses: we're assuming that those affiliated with any hospital don't really decide whether or not they want to use EHR so we can't look at the effects of their demographics on the choice of EHR usage, instead we will look at the demographics of their affiliated hospitals; and focus on the analyzing the effects of practitioner demographics on the choice of EHR usage among those who are not affiliated with any hospitals.

```{r}
# those with some kind of hospital affiliation
hosp <- unique_dat %>% filter(Hospital.affiliation.CCN.1 != "" | Hospital.affiliation.CCN.2 != "" | Hospital.affiliation.CCN.3 != "" | Hospital.affiliation.CCN.4 != "" | !is.na(Hospital.affiliation.CCN.5))
saveRDS(hosp, file="hosp.rds")

# those without any hospital affiliation
phys <- unique_dat %>% 
  filter(Hospital.affiliation.CCN.1 == "" & Hospital.affiliation.CCN.2 == "" & Hospital.affiliation.CCN.3 == "" & Hospital.affiliation.CCN.4 == "" & is.na(Hospital.affiliation.CCN.5)) %>%
  # filter_at(vars(starts_with("Hospital")), all_vars(. == "")) %>% 
  # filter(is.na(Hospital.affiliation.CCN.5)) %>% # CCN5 is the only one that's integer so has NAs instead of blanks
  select(-starts_with("Hospital.affiliation"))
saveRDS(phys, file="phys.rds")
```














